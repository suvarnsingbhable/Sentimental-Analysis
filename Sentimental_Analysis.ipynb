# Install Library and import  
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split  #for model
import matplotlib.pyplot as plt #visulation

from keras.models import Sequential, load_model
from keras.layers import Dense, LSTM, Embedding, Dropout
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences

#load Dataset
data = pd.read_csv('/home/sing/Downloads/twitter-airline-sentiment/Tweets.csv') 
data = data.sample(frac=1).reset_index(drop=True)
print(data.shape)
data.head()

data = data[['airline_sentiment', 'text']]
data.head()

data['airline_sentiment'].value_counts().sort_index().plot.bar() #visulation

data['text'].str.len().plot.hist() # Histogram

#preprocessing
data['text'] = data['text'].str.replace('@VirginAmerica', '')
data.head()

#transform text to lowercase
data['text'].apply(lambda x: x.lower()) 

#Regular Expression
import re
data['text'] = data['text'].apply(lambda x: re.sub('[^a-zA-z0-9\s]', '', x))
data['text'].head()

#tokenization
tokenizer = Tokenizer(num_words=5000, split=" ")
tokenizer.fit_on_texts(data['text'].values)

X = tokenizer.texts_to_sequences(data['text'].values)
X = pad_sequences(X) # padding our text vector so they all have the same length
X[:5]

#creating model
model = Sequential()
model.add(Embedding(5000, 256, input_length=X.shape[1]))
model.add(Dropout(0.3))
model.add(LSTM(256, return_sequences=True, dropout=0.3, recurrent_dropout=0.2))
model.add(LSTM(256, dropout=0.3, recurrent_dropout=0.2))
model.add(Dense(3, activation='softmax'))

model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
model.summary()

y = pd.get_dummies(data['airline_sentiment']).values
[print(data['airline_sentiment'][i], y[i]) for i in range(0,5)]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

model.save('sentiment_analysis.h1')

#TEsting Model
predictions = model.predict(X_test)

[print(data['text'][i], predictions[i], y_test[i]) for i in range(0, 5)]


pos_count, neu_count, neg_count = 0, 0, 0
real_pos, real_neu, real_neg = 0, 0, 0
for i, prediction in enumerate(predictions):
    if np.argmax(prediction)==2:
        pos_count += 1
    elif np.argmax(prediction)==1:
        neu_count += 1
    else:
        neg_count += 1
    
    if np.argmax(y_test[i])==2:
        real_pos += 1
    elif np.argmax(y_test[i])==1:    
        real_neu += 1
    else:
        real_neg +=1

print('Positive predictions:', pos_count)
print('Neutral predictions:', neu_count)
print('Negative predictions:', neg_count)
print('Real positive:', real_pos)
print('Real neutral:', real_neu)
print('Real negative:', real_neg)
